{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b17f3a-fc40-43d2-b026-798f26014493",
   "metadata": {},
   "source": [
    "# <center> News Classification with NLP and Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb09f44-33f4-4938-aaa4-65c3df93b527",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1634b-c53c-4c99-8e6e-f7759bd2856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "import plotly.graph_objects as go\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a02e18-a47b-4179-9c28-fb2bb93cc5d6",
   "metadata": {},
   "source": [
    "### Data\n",
    "- Import JSON file\n",
    "- Limit to top categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebb1ae-25d6-43d9-9510-54dd2e864b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in json file to DataFrame\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "\n",
    "# View Results\n",
    "print(df.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd82dfc-6153-4f97-ad75-9007960b828b",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca86a7-4e67-4e77-839e-0a2ffbc6920f",
   "metadata": {},
   "source": [
    "#### Check data types"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e41005e-96ba-4cc5-b7e5-78f99bceae05",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314368a0-6cfb-4d84-8e6a-53b17873c0f8",
   "metadata": {},
   "source": [
    "#### Check NaNs "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a6edbf1-c01f-4b57-a18d-5e58be8a9fb7",
   "metadata": {},
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601100b7-6574-403b-9df5-b7009c95b3af",
   "metadata": {},
   "source": [
    "#### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4a10b-d5c2-4811-b945-5ebd596a25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {df.duplicated().sum()} duplicated rows\")\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f\"There are now {df.duplicated().sum()} duplicated rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bb9ac-6a5a-4abc-ae61-82cc5e5282f2",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922624b-8ddb-44c4-8648-00f0e807c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba9ece-1396-4aaf-b4fe-248728420b19",
   "metadata": {},
   "source": [
    "#### Authors \n",
    "- The `authors` field is a list containing:\n",
    "    - Name(s)\n",
    "    - Titles\n",
    "    - Organizations\n",
    "    - Misc comments\n",
    "- It also contains many NaNs in the form of empty strings\n",
    "- Approach:\n",
    "    - Replace NaNs with 'unknown'\n",
    "    - Extract author names from the field, create new field named `author_names` to be used as a bigram\n",
    "    - Leave the rest of the information in a new field named `author_notes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546711d-3f03-4745-b91c-4eb5ca22438b",
   "metadata": {},
   "source": [
    "### Author Names\n",
    "- Replace missing\n",
    "- Get Names\n",
    "- Clean non-name details\n",
    "- Cast as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f923fa-b7e9-41ea-99b3-1fa83c25f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing authors with 'unknown'\n",
    "df['authors'] = df['authors'].apply(lambda x: x.replace('','unknown') if x == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5277caf-7482-47ad-a150-8f78b701fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get author names from list\n",
    "df['author_names'] = df['authors'].apply(lambda x: x.replace('By','').strip().split(',')[0].lower().split(' And '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb2e53-c1e0-4d56-a942-4ae27986b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up common formatting issues\n",
    "df['authors'] = df['authors'].apply(lambda x: x.replace('By','').replace('\\n','').replace('Contributor','Contributor ').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85739996-443e-4d95-8cbc-5eb9ff0cc15c",
   "metadata": {},
   "source": [
    "### Links\n",
    "- The links are not helpful in their current form, need to extract keyworks from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dd769-3f83-488f-a04e-8b86eab29beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace charaters we want to split on with commas, then split, only get the relevant entries from the resulting list\n",
    "df['link_keywords'] = df['link'].apply(lambda x: x.replace('-',',').replace(\"_\",',').replace(\"entry/\",',').split(',')[1:-2])\n",
    "\n",
    "# Typecast from list to string\n",
    "df['link_keywords'] = df['link_keywords'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ff202-4315-4e44-bd04-707dd47962df",
   "metadata": {},
   "source": [
    "### Join all text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df868d-a8fb-49d2-962d-3ff44d3c11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['headline'] + ' ' + df['short_description'] + ' ' + df['link_keywords'] + ' ' + df['authors'] + ' ' + df['date'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6ea61-e042-41bd-8c32-79609de50c10",
   "metadata": {},
   "source": [
    "### Drop unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba9bb9-a3b4-4265-b6ed-8871b2cbfbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the feature if it appear in the colums (done this way so the cell can be re-run)\n",
    "df = df.drop(columns=[col for col in df.columns if col in ['link','authors','headline','short_description','date','link_keywords','author_notes','author_names']])\n",
    "\n",
    "# View Sample\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb007b6-d2dc-4d7c-b402-c82261924282",
   "metadata": {},
   "source": [
    "# <center> -------------------------------------------------------------------- </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d018a1-5899-4858-b61e-56ff7e756afc",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba73be-a3c7-4dd0-9743-1053b30d65e2",
   "metadata": {},
   "source": [
    "#### View categories"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbe5b261-a6fc-45e6-a74b-9aa23c4f4f7d",
   "metadata": {},
   "source": [
    "print(f\"There are {len(df['category'].value_counts())} unique categories including the following:\")\n",
    "px.bar(df['category'].value_counts(), \n",
    "       title='Unique New Categories', \n",
    "       labels = {\"value\": \"Number of Articles\",\"index\": \"Category\"},\n",
    "       width = 800, height = 450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d18038-8d52-4e38-8b50-0985920ea7a2",
   "metadata": {},
   "source": [
    "#### View length of texts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f41dda5d-1cf5-4053-82bd-af4c69d0a770",
   "metadata": {},
   "source": [
    "headline_lengths = df['text'].apply(lambda x: len(x))\n",
    "px.histogram(df, \n",
    "             x = headline_lengths, \n",
    "             marginal = 'box', \n",
    "             title = 'Text Lengths',\n",
    "             labels = {'x':'Number of Characters'},\n",
    "             color = 'category',\n",
    "             height = 500, width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c27f96-acfd-433d-b758-6c0159e2e48f",
   "metadata": {},
   "source": [
    "### Author Activity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fff23eef-0738-4aed-9562-6795cc5908c9",
   "metadata": {},
   "source": [
    "unique_authors_vc = df['author_names'].explode().value_counts()\n",
    "\n",
    "print(f\"There are {len(unique_authors_vc)} unique authors, {unique_authors_vc[0]} ({round(unique_authors_vc[0]/len(unique_authors_vc),2)}%) are unknown.\" )\n",
    "\n",
    "px.bar(unique_authors_vc[1:25], \n",
    "       title='Unique Authors', \n",
    "       labels = {\"value\": \"Number of Articles Written\",\"index\": \"Author\"},\n",
    "       width = 1200, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d17d54-13d9-48d7-85c6-a45df5510091",
   "metadata": {},
   "source": [
    "# <center> -------------------------------------------------------------------- </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28573ff1-d3fe-4385-872c-13d429c28972",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data\n",
    "- Stop words\n",
    "- Stem / Lemmatize text\n",
    "- Tokenization\n",
    "- Use all words or just most frequent?\n",
    "- Use bigrams, POS taggins, Mutual information Scores?\n",
    "- What sort of vectorization? (Boolean / Count / TF-IDF / Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71728c1f-5d52-4f52-87c6-a34dd89b5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61bb42-eb7b-46c6-a310-d44f96b94738",
   "metadata": {},
   "source": [
    "### Encode Target Variable\n",
    "- Assign a unique value to each categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7788c7-8872-4c7f-9894-07bddc823cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate label encoder \n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Label Encode categories\n",
    "df['class_label'] = le.fit_transform(df['category'])\n",
    "\n",
    "# Create dictionary to map labels to categories\n",
    "label_to_category = dict(df.groupby('class_label')['category'].first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8def016-e2ec-4835-ae6e-2320bd051b35",
   "metadata": {},
   "source": [
    "### Get Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c31304-040c-4970-9482-818a2a8fb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c14a8a-2002-41f9-93c7-c58d2ce6b7e9",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755edad4-1701-46ef-bbbf-335613479515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dependent and independent variables\n",
    "X = df[\"text\"]\n",
    "y = df[\"class_label\"]\n",
    "\n",
    "# Perform Test Train Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da3b01-4919-411b-b7fa-4cb5671c3f9a",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c25d8-5546-40d3-87a9-80812c967086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer with custom list of stop words\n",
    "count_vectorizer = CountVectorizer(stop_words= stopwords_list)\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "count_vectorizer.fit(X_train)\n",
    "\n",
    "# Apply the tokenizer to the training and testing data\n",
    "X_train_counts = count_vectorizer.transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "X_val_counts = count_vectorizer.transform(X_val)\n",
    "\n",
    "# View Results\n",
    "print(f'There are {X_train_counts.shape[0]} observations and {X_train_counts.shape[1]} features')\n",
    "pd.DataFrame(data= X_val_counts[:5].toarray(), columns = count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb0370-57f3-4dd8-8fa6-f6513458070e",
   "metadata": {},
   "source": [
    "## Model: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996917c-12b6-4ee1-a20c-d176252d206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f333256-39d7-470a-8e27-7ba35151f19a",
   "metadata": {},
   "source": [
    "### OHE target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1164e5-684c-44e0-98b6-dd064ff52aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# OHE for netural network\n",
    "y_train_ohe = to_categorical(y_train)\n",
    "y_val_ohe = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23067729-838a-46e0-8927-533142ff8157",
   "metadata": {},
   "source": [
    "### Create model\n",
    "- Fully connected (dense) layer network with relu activation\n",
    "- 2 hidden layers with 50 units in 1st and 25 in second\n",
    "- Softmax classifier for nulticlass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d0525-ac0a-4242-946c-d970618aad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# One layer with softmax activation \n",
    "model.add(layers.Dense(41, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d315a6b-bb33-4caf-ac75-ab889cab5673",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "- Loss function = categorical crossentropy\n",
    "- Optimizer = stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf285352-5a66-4d9e-a7bb-6b36af673c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef68727-0ba3-4ca0-bf57-144eee96cadb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model \n",
    "n_epochs = 10\n",
    "history = model.fit(X_train_counts, y_train_ohe, epochs= n_epochs, validation_data = (X_val_counts, y_val_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca0c7c-9b25-41e3-bcac-3cf7c61aca01",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933323-5e79-4b08-b536-84da04aa0b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_model_metrics(actuals, predictions, full_report = False, plot = False):  \n",
    "\n",
    "    predictions_bool = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision = round(precision_score(actuals, predictions_bool, average='weighted'),3)\n",
    "    recall = round(recall_score(actuals, predictions_bool, average='weighted'),3)\n",
    "    f1 = round(f1_score(actuals, predictions_bool, average='weighted'),3)\n",
    "    accuracy = round(accuracy_score(actuals, predictions_bool),3)\n",
    "    \n",
    "    if full_report == False:\n",
    "        print(f\"Accuracy = {accuracy} \\nPrecision = {precision} \\nRecall = {recall} \\nf1 = {f1}\")\n",
    "    else:\n",
    "        print(classification_report(actuals, predictions_bool))\n",
    "    if plot == True:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=[i for i in range(20)], y=history.history['acc'],\n",
    "                            mode='lines+markers', name='Train Accuracy'))\n",
    "        fig.add_trace(go.Scatter(x=[i for i in range(20)], y=history.history['val_acc'],\n",
    "                            mode='lines+markers', name='Validation Accuracy'))\n",
    "\n",
    "        fig.update_layout(height= 500, width= 700)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a342fc8-3e01-402f-9e72-dcb1a2f0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(y_val, model.predict(X_val_counts), full_report = 0, plot = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7788a2d-bb6b-4f90-b552-514e7a0eddf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
